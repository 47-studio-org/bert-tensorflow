FIRST LONG BERT

python3 run_pretraining.py \
 --input_file=${BERT_BASE_DIR}/training_data/tf_examples.tfrecord1 \
 --output_dir=${BERT_BASE_DIR}/${RUN_DIR} \
 --do_train=True \
 --do_eval=True \
 --bert_config_file=${BERT_BASE_DIR}/bert_config.json \
 --train_batch_size=1024 \
 --max_seq_length=128 \
 --max_predictions_per_seq=20 \
 --num_train_steps=500000 \
 --num_warmup_steps=10 \
 --learning_rate=1e-4 \
 --use_tpu=True \
 --tpu_name=bert\
 --save_checkpoints_steps=3500\
 --iterations_per_loop=500\
 --save_summary_steps=500


4000 * 1024
35K batches in 8:30 hrs
About 4k batches per hour
4 mill samples per hour

--------------------------------------------------------------
--------------------------------------------------------------

Our Small Set

approx 4million samples in one .tfrecords file


python3 run_pretraining.py \
 --input_file=${BERT_BASE_DIR}/training_data/tf_examples.tfrecord1 \
 --output_dir=${BERT_BASE_DIR}/${RUN_DIR} \
 --do_train=True \
 --do_eval=True \
 --bert_config_file=${BERT_BASE_DIR}/bert_config.json \
 --train_batch_size=8 \
 --max_seq_length=128 \
 --use_tpu=True \
 --max_predictions_per_seq=20 \
 --num_train_steps=30000 \
 --num_warmup_steps=1000 \
 --learning_rate=2e-5 \
 --tpu_name=bert\
 --save_checkpoints_steps=500\
 --iterations_per_loop=1\
 --save_summary_steps=1



 --------------------------------------------------------------
--------------------------------------------------------------

TOY SET

python create_pretraining_data.py \
  --input_file=./samples/sample_text.txt \
  --output_file=./samples/sample_text.tfrecord \
  --vocab_file=./samples/vocab.txt \
  --do_lower_case=True \
  --max_seq_length=128 \
  --max_predictions_per_seq=20 \
  --masked_lm_prob=0.15 \
  --random_seed=12345 \
  --dupe_factor=20

 python3 run_pretraining.py \
 --input_file=${BERT_BASE_DIR}/training_data/sample_text.tfrecord \
 --output_dir=${BERT_BASE_DIR}/${RUN_DIR} \
 --do_train=True \
 --do_eval=True \
 --bert_config_file=${BERT_BASE_DIR}/bert_config.json \
 --train_batch_size=32 \
 --max_seq_length=128 \
 --max_predictions_per_seq=20 \
 --num_train_steps=1000 \
 --num_warmup_steps=100 \
 --learning_rate=2e-5 \
 --use_tpu=True \
 --tpu_name=bert\
 --save_checkpoints_steps=200\
 --iterations_per_loop=10\
 --save_summary_steps=10


  --------------------------------------------------------------
--------------------------------------------------------------

DATA SHUFFLE TEST

python create_pretraining_data.py \
  --input_file=./samples/der.txt \
  --output_file=./samples/der.tfrecord \
  --vocab_file=./samples/vocab.txt \
  --do_lower_case=True \
  --max_seq_length=128 \
  --max_predictions_per_seq=20 \
  --masked_lm_prob=0.15 \
  --random_seed=12345 \
  --dupe_factor=1

python create_pretraining_data.py \
  --input_file=./samples/sample_text.txt \
  --output_file=./samples/sample_text_d1.tfrecord \
  --vocab_file=./samples/vocab.txt \
  --do_lower_case=True \
  --max_seq_length=128 \
  --max_predictions_per_seq=20 \
  --masked_lm_prob=0.15 \
  --random_seed=12345 \
  --dupe_factor=1

python3 run_pretraining.py \
 --input_file=./samples/sample_text_d1.tfrecord\
 --output_dir=./runs/sample_test/ \
 --do_train=True \
 --do_eval=True \
 --bert_config_file=./samples/bert_config.json \
 --train_batch_size=1 \
 --max_seq_length=128 \
 --max_predictions_per_seq=20 \
 --num_train_steps=200 \
 --num_warmup_steps=10 \
 --learning_rate=2e-5 \
 --save_checkpoints_steps=200\
 --iterations_per_loop=1\
 --save_summary_steps=1


  --------------------------------------------------------------
--------------------------------------------------------------

python3 run_pretraining.py \
 --input_file=./samples/sample_text_d1.tfrecord\
 --output_dir=./runs/logging/ \
 --do_train=True \
 --do_eval=True \
 --bert_config_file=./samples/bert_config.json \
 --train_batch_size=1 \
 --max_seq_length=128 \
 --max_predictions_per_seq=20 \
 --num_train_steps=200 \
 --num_warmup_steps=10 \
 --learning_rate=2e-5 \
 --save_checkpoints_steps=200\
 --iterations_per_loop=1\
 --save_summary_steps=1

--------------------------------------------------------------
--------------------------------------------------------------

 EVAL

python3 run_pretraining.py \
 --input_file=./samples/sample_text.tfrecord\
 --output_dir=./runs/eval_loop_2/ \
 --do_train=True \
 --do_eval=False \
 --bert_config_file=./samples/bert_config.json \
 --train_batch_size=1 \
 --max_seq_length=128 \
 --max_predictions_per_seq=20 \
 --num_train_steps=200 \
 --num_warmup_steps=10 \
 --learning_rate=2e-5 \
 --save_checkpoints_steps=25\
 --iterations_per_loop=1\
 --save_summary_steps=1

Local

--ckpt_dir=./runs/eval_loop
--input_files=./samples/sample_text.tfrecord
--bert_config_file=./samples/bert_config.json
--eval_batch_size=1
--max_seq_length=128
--iterations_per_loop=8
--max_eval_steps=32

