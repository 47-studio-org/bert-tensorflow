FIRST LONG BERT

python3 run_pretraining.py \
 --input_file=${BERT_BASE_DIR}/training_data/tf_examples.tfrecord1 \
 --output_dir=${BERT_BASE_DIR}/${RUN_DIR} \
 --do_train=True \
 --do_eval=True \
 --bert_config_file=${BERT_BASE_DIR}/bert_config.json \
 --train_batch_size=1024 \
 --max_seq_length=128 \
 --max_predictions_per_seq=20 \
 --num_train_steps=500000 \
 --num_warmup_steps=10 \
 --learning_rate=1e-4 \
 --use_tpu=True \
 --tpu_name=bert\
 --save_checkpoints_steps=3500\
 --iterations_per_loop=500\
 --save_summary_steps=500


4000 * 1024
35K batches in 8:30 hrs
About 4k batches per hour
4 mill samples per hour

--------------------------------------------------------------
--------------------------------------------------------------

Our Small Set

approx 4million samples in one .tfrecords file


python3 run_pretraining.py \
 --input_file=${BERT_BASE_DIR}/training_data/tf_examples.tfrecord1 \
 --output_dir=${BERT_BASE_DIR}/${RUN_DIR} \
 --do_train=True \
 --do_eval=True \
 --bert_config_file=${BERT_BASE_DIR}/bert_config.json \
 --train_batch_size=1024 \
 --max_seq_length=128 \
 --max_predictions_per_seq=20 \
 --num_train_steps=30000 \
 --num_warmup_steps=1000 \
 --learning_rate=2e-5 \
 --use_tpu=True \
 --tpu_name=bert\
 --save_checkpoints_steps=500\
 --iterations_per_loop=50\
 --save_summary_steps=50


 --------------------------------------------------------------
--------------------------------------------------------------

TOY SET

python create_pretraining_data.py \
  --input_file=./samples/sample_text.txt \
  --output_file=./samples/sample_text.tfrecord \
  --vocab_file=./samples/vocab.txt \
  --do_lower_case=True \
  --max_seq_length=128 \
  --max_predictions_per_seq=20 \
  --masked_lm_prob=0.15 \
  --random_seed=12345 \
  --dupe_factor=20

 python3 run_pretraining.py \
 --input_file=${BERT_BASE_DIR}/training_data/sample_text.tfrecord \
 --output_dir=${BERT_BASE_DIR}/${RUN_DIR} \
 --do_train=True \
 --do_eval=True \
 --bert_config_file=${BERT_BASE_DIR}/bert_config.json \
 --train_batch_size=32 \
 --max_seq_length=128 \
 --max_predictions_per_seq=20 \
 --num_train_steps=1000 \
 --num_warmup_steps=100 \
 --learning_rate=2e-5 \
 --use_tpu=True \
 --tpu_name=bert\
 --save_checkpoints_steps=200\
 --iterations_per_loop=10\
 --save_summary_steps=10